{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.11","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":11833570,"sourceType":"datasetVersion","datasetId":7434328},{"sourceId":11833579,"sourceType":"datasetVersion","datasetId":7434335}],"dockerImageVersionId":31040,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"!pip install emoji contractions\n!pip install lightgbm","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-06-08T10:59:07.483212Z","iopub.execute_input":"2025-06-08T10:59:07.483846Z","iopub.status.idle":"2025-06-08T10:59:18.740089Z","shell.execute_reply.started":"2025-06-08T10:59:07.483816Z","shell.execute_reply":"2025-06-08T10:59:18.738894Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nimport joblib\nimport re\nimport emoji\nimport contractions\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nimport random\nfrom sklearn.metrics import classification_report, accuracy_score, f1_score, confusion_matrix\nfrom sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nfrom sklearn.multioutput import MultiOutputRegressor\nfrom scipy.sparse import hstack\nfrom scipy.stats import pearsonr\nfrom collections import defaultdict\n\nfrom sklearn.linear_model import LogisticRegression, Ridge\nfrom sklearn.multioutput import MultiOutputRegressor\nfrom sklearn.multiclass import OneVsRestClassifier\nfrom lightgbm import LGBMRegressor\nfrom lightgbm import LGBMClassifier","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-08T10:59:18.742826Z","iopub.execute_input":"2025-06-08T10:59:18.743160Z","iopub.status.idle":"2025-06-08T10:59:26.090117Z","shell.execute_reply.started":"2025-06-08T10:59:18.743127Z","shell.execute_reply":"2025-06-08T10:59:26.089058Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Data Preparation","metadata":{}},{"cell_type":"code","source":"# Load datasets\ntrain = pd.read_csv('/kaggle/input/elreg-datasets/train.csv', delimiter='\\t')\ntest = pd.read_csv('/kaggle/input/elreg-datasets/test.csv', delimiter='\\t')\ndev = pd.read_csv('/kaggle/input/elreg-datasets/dev.csv', delimiter='\\t')\n\ncombined_df = pd.concat([train, dev, test], ignore_index=True)\n\n# Atur parameter test_size di Streamlit\ntrain_df, test_df = train_test_split(combined_df, test_size=0.2, random_state=42)\n\nprint(f\"Training set: {train_df.shape}\")\nprint(f\"Test set: {test_df.shape}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-08T10:59:26.091240Z","iopub.execute_input":"2025-06-08T10:59:26.091969Z","iopub.status.idle":"2025-06-08T10:59:26.221011Z","shell.execute_reply.started":"2025-06-08T10:59:26.091932Z","shell.execute_reply":"2025-06-08T10:59:26.219386Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Data Cleaning","metadata":{}},{"cell_type":"code","source":"# Text preprocessing functions\ndef convert_emojis(text):\n    text = emoji.demojize(text, delimiters=(\" \", \" \"))\n    text = re.sub(r':([a-zA-Z_]+):', r'\\1', text)\n    text = re.sub(r'\\s+', ' ', text).strip()\n    return text\n\ndef clean_text(text):\n    # Lowercase\n    text = text.lower()\n    # expand contractions\n    text = contractions.fix(text)\n    # convert emojis\n    text = convert_emojis(text)\n    # Remove URLs\n    text = re.sub(r\"http\\S+|www\\S+|https\\S+\", '', text, flags=re.MULTILINE)\n    # Remove user mentions\n    text = re.sub(r'@\\w+', '', text)\n    # Remove special characters and numbers (except punctuation)\n    text = re.sub(r\"[^a-zA-Z\\s.,!?']\", '', text)\n    # Remove extra spaces\n    text = re.sub(r'\\s+', ' ', text).strip()\n    return text\n\n# Apply text cleaning\ntrain_df[\"clean_text\"] = train_df[\"Tweet\"].apply(clean_text)\ntest_df[\"clean_text\"] = test_df[\"Tweet\"].apply(clean_text)\n\n# Define emotion columns\nemotion_cols = [\"joy\", \"sadness\", \"anger\", \"fear\"]","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-08T10:59:26.222524Z","iopub.execute_input":"2025-06-08T10:59:26.223303Z","iopub.status.idle":"2025-06-08T10:59:27.560232Z","shell.execute_reply.started":"2025-06-08T10:59:26.223263Z","shell.execute_reply":"2025-06-08T10:59:27.559170Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Check for missing values in emotion columns\nprint(\"Missing values in emotion columns:\")\nfor df, name in [(train_df, \"train\"), (test_df, \"test\")]:\n    print(f\"\\n{name} dataset:\")\n    for col in emotion_cols:\n        missing = df[col].isna().sum()\n        total = len(df)\n        print(f\"{col}: {missing} missing values ({missing/total*100:.1f}%)\")\n\n# Fill missing values with 0 (indicating absence of that emotion)\nfor df in [train_df, test_df]:\n    for col in emotion_cols:\n        df[col] = df[col].fillna(0.0)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-08T10:59:27.561248Z","iopub.execute_input":"2025-06-08T10:59:27.561516Z","iopub.status.idle":"2025-06-08T10:59:27.575801Z","shell.execute_reply.started":"2025-06-08T10:59:27.561494Z","shell.execute_reply":"2025-06-08T10:59:27.574841Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Explanatory Data Analysis (EDA)","metadata":{}},{"cell_type":"code","source":"emotion_counts = (train_df[emotion_cols] > 0).sum()\n\nsns.set(style=\"whitegrid\")\nplt.figure(figsize=(6,4))\nax = sns.barplot(x=emotion_counts.index, y=emotion_counts.values, palette=\"muted\")\n\nfor i, count in enumerate(emotion_counts.values):\n    ax.text(i, count, str(count), ha=\"center\", va=\"bottom\")\n\nplt.title(\"Emotion Distribution\")\nplt.xlabel(\"Emotion\")\nplt.ylabel(\"Count\")\nplt.tight_layout()\nplt.show()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-08T10:59:27.577043Z","iopub.execute_input":"2025-06-08T10:59:27.577531Z","iopub.status.idle":"2025-06-08T10:59:27.952612Z","shell.execute_reply.started":"2025-06-08T10:59:27.577506Z","shell.execute_reply":"2025-06-08T10:59:27.951612Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Sample of High and Low Intensity Emotion\ndef sample_emotion_extremes(df, emotion, high_thresh=0.9, low_thresh=0.1, n=3):\n    print(f\"\\nðŸ”¹ Emotion: {emotion.upper()}\")\n\n    high = df[df[emotion] >= high_thresh].sample(n=min(n, len(df[df[emotion] >= high_thresh])))\n    print(f\"\\nHigh Intensity (â‰¥ {high_thresh}):\")\n    for i, row in high.iterrows():\n        print(f\"  - ({row[emotion]:.2f}) {row['clean_text']}\")\n\n    print(\"\\n\")\n    \n    low = df[(df[emotion] < low_thresh) & (df[emotion] > 0)].sample(n=min(n, len(df[(df[emotion] < low_thresh) & (df[emotion] > 0)])))\n    print(f\"\\nLow Intensity (> 0 and < {low_thresh}):\")\n    for i, row in low.iterrows():\n        print(f\"  - ({row[emotion]:.2f}) {row['clean_text']}\")\n\n    print(\"-----------------------------------------------------------------------------\")\n\n# Show for each emotion\nfor emo in emotion_cols:\n    sample_emotion_extremes(df, emo)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-08T10:59:27.956741Z","iopub.execute_input":"2025-06-08T10:59:27.957102Z","iopub.status.idle":"2025-06-08T10:59:27.986154Z","shell.execute_reply.started":"2025-06-08T10:59:27.957077Z","shell.execute_reply":"2025-06-08T10:59:27.985215Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Lexicons","metadata":{}},{"cell_type":"code","source":"# Load EmoLex features\ndef load_lex(filepath):\n    lexicon = defaultdict(dict)\n    with open(filepath, 'r') as file:\n        for line in file:\n            word, emotion, value = line.strip().split('\\t')\n            if int(value) == 1:\n                lexicon[word][emotion] = 1\n    return lexicon\n\nnrc_lexicon = load_lex(\"/kaggle/input/nrc-lexicons/NRC-Emotion-Lexicon-Wordlevel-v0.92.txt\")\n\ndef extract_lex(text, lexicon):\n    emotions = ['anger', 'anticipation', 'disgust', 'fear', 'joy',\n              'sadness', 'surprise', 'trust', 'positive', 'negative']\n    counts = dict.fromkeys(emotions, 0)\n\n    for word in text.split():\n        if word in lexicon:\n            for emo in lexicon[word]:\n                counts[emo] += 1\n    return [counts[emo] for emo in emotions]\n\n# Extract lexicon features\ntrain_df['lexicons'] = train_df['clean_text'].apply(lambda x: extract_lex(x, nrc_lexicon))\ntest_df['lexicons'] = test_df['clean_text'].apply(lambda x: extract_lex(x, nrc_lexicon))\n\ntrain_lex = np.array(train_df['lexicons'].tolist())\ntest_lex = np.array(test_df['lexicons'].tolist())","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-08T10:59:27.987065Z","iopub.execute_input":"2025-06-08T10:59:27.987435Z","iopub.status.idle":"2025-06-08T10:59:28.333764Z","shell.execute_reply.started":"2025-06-08T10:59:27.987411Z","shell.execute_reply":"2025-06-08T10:59:28.332844Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Load VAD Lexicons\ndef load_nrc_vad(filepath):\n    vad_lex = {}\n    with open(filepath, 'r', encoding='utf-8') as f:\n        next(f)  # skip header\n        for line in f:\n            word, val, aro, dom = line.strip().split('\\t')\n            vad_lex[word] = {\n                'valence': float(val),\n                'arousal': float(aro),\n                'dominance': float(dom)\n            }\n    return vad_lex\n\nnrc_vad_lexicon = load_nrc_vad(\"/kaggle/input/nrc-lexicons/NRC-VAD-Lexicon-v2.1.txt\")\n\ndef extract_vad(text, lexicon):\n    valence = []\n    arousal = []\n    dominance = []\n\n    for word in text.split():\n        if word in lexicon:\n            valence.append(lexicon[word]['valence'])\n            arousal.append(lexicon[word]['arousal'])\n            dominance.append(lexicon[word]['dominance'])\n\n    # If no word matched, return zeros\n    if not valence:\n        return [0.0, 0.0, 0.0]\n\n    # Otherwise, return means\n    return [\n        np.mean(valence),\n        np.mean(arousal),\n        np.mean(dominance)\n    ]\n\n# Extract lexicon features\ntrain_df['vad'] = train_df['clean_text'].apply(lambda x: extract_vad(x, nrc_vad_lexicon))\ntest_df['vad'] = test_df['clean_text'].apply(lambda x: extract_vad(x, nrc_vad_lexicon))\n\ntrain_vad = np.array(train_df['vad'].tolist())\ntest_vad = np.array(test_df['vad'].tolist())","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-08T10:59:28.334826Z","iopub.execute_input":"2025-06-08T10:59:28.335207Z","iopub.status.idle":"2025-06-08T10:59:28.822411Z","shell.execute_reply.started":"2025-06-08T10:59:28.335177Z","shell.execute_reply":"2025-06-08T10:59:28.821229Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Load HashEmo Lexicons\ndef load_nrc_hash_emo(filepath):\n    lexicon = defaultdict(dict)\n    with open(filepath, 'r', encoding='utf-8') as f:\n        for line in f:\n            emotion, word, score = line.strip().split('\\t')\n            lexicon[word][emotion] = float(score)\n    return lexicon\n\nhash_emo_lex = load_nrc_hash_emo('/kaggle/input/nrc-lexicons/NRC-Hashtag-Emotion-Lexicon-v0.2.txt')\n\ndef extract_hash_emo(text, lexicon):\n    emotions = ['anger', 'anticipation', 'disgust', 'fear', 'joy',\n                'sadness', 'surprise', 'trust']\n    scores = {emo: [] for emo in emotions}\n\n    for word in text.split():\n        if word in lexicon:\n            for emo, value in lexicon[word].items():\n                scores[emo].append(value)\n\n    return [np.mean(scores[emo]) if scores[emo] else 0.0 for emo in emotions]\n\ntrain_df['hash'] = train_df['clean_text'].apply(lambda x: extract_hash_emo(x, hash_emo_lex))\ntest_df['hash'] = test_df['clean_text'].apply(lambda x: extract_hash_emo(x, hash_emo_lex))\n\ntrain_hash = np.array(train_df['hash'].tolist())\ntest_hash = np.array(test_df['hash'].tolist())","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-08T10:59:28.823505Z","iopub.execute_input":"2025-06-08T10:59:28.823832Z","iopub.status.idle":"2025-06-08T10:59:29.581987Z","shell.execute_reply.started":"2025-06-08T10:59:28.823810Z","shell.execute_reply":"2025-06-08T10:59:29.581092Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"scaler_hash = StandardScaler()\ntrain_hash = scaler_hash.fit_transform(train_hash)\ntest_hash = scaler_hash.transform(test_hash)\n\nscaler_lex = StandardScaler()\ntrain_lex = scaler_lex.fit_transform(train_lex)\ntest_lex = scaler_lex.transform(test_lex)\n\nscaler_vad = StandardScaler()\ntrain_vad = scaler_vad.fit_transform(train_vad)\ntest_vad = scaler_vad.transform(test_vad)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-08T10:59:29.582891Z","iopub.execute_input":"2025-06-08T10:59:29.583289Z","iopub.status.idle":"2025-06-08T10:59:29.627044Z","shell.execute_reply.started":"2025-06-08T10:59:29.583259Z","shell.execute_reply":"2025-06-08T10:59:29.626013Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# NRC Hash-Emo + EmoLex + VAD\ntrain_combined = np.concatenate([train_vad, train_lex, train_hash], axis=1)\ntest_combined = np.concatenate([test_vad, test_lex, test_hash], axis=1)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-08T10:59:29.628484Z","iopub.execute_input":"2025-06-08T10:59:29.628767Z","iopub.status.idle":"2025-06-08T10:59:29.634033Z","shell.execute_reply.started":"2025-06-08T10:59:29.628746Z","shell.execute_reply":"2025-06-08T10:59:29.633078Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Extract Embeddings","metadata":{}},{"cell_type":"code","source":"# TF-IDF Vectorization\ntfidf = TfidfVectorizer(\n    max_features=5000,\n    ngram_range=(1, 2),\n    stop_words='english'\n)\n\n# Fit on training set and transform all sets\ntrain_tfidf = tfidf.fit_transform(train_df['clean_text'])\ntest_tfidf = tfidf.transform(test_df['clean_text'])","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-08T10:59:29.635114Z","iopub.execute_input":"2025-06-08T10:59:29.635428Z","iopub.status.idle":"2025-06-08T10:59:30.105785Z","shell.execute_reply.started":"2025-06-08T10:59:29.635404Z","shell.execute_reply":"2025-06-08T10:59:30.104380Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Training Preparation","metadata":{}},{"cell_type":"code","source":"# TF-IDF is sparse, so we use hstack (sparse-safe)\nX_train = hstack([train_tfidf, train_combined])\nX_test = hstack([test_tfidf, test_combined])","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-08T10:59:30.107143Z","iopub.execute_input":"2025-06-08T10:59:30.107399Z","iopub.status.idle":"2025-06-08T10:59:30.128731Z","shell.execute_reply.started":"2025-06-08T10:59:30.107382Z","shell.execute_reply":"2025-06-08T10:59:30.127745Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# y_train_cls = (train_df[emotion_cols] > 0).astype(int).values\n# y_test_cls  = (test_df[emotion_cols] > 0).astype(int).values\n\ny_train_cls = train_df[emotion_cols].values.argmax(axis=1)\ny_test_cls = test_df[emotion_cols].values.argmax(axis=1)\n\nlabel_map = {0: 'joy', 1: 'sadness', 2: 'anger', 3: 'fear'}\n\ny_train_reg = train_df[emotion_cols].values\ny_test_reg = test_df[emotion_cols].values","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-08T10:59:30.129925Z","iopub.execute_input":"2025-06-08T10:59:30.130193Z","iopub.status.idle":"2025-06-08T10:59:30.151046Z","shell.execute_reply.started":"2025-06-08T10:59:30.130173Z","shell.execute_reply":"2025-06-08T10:59:30.149739Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Regression Model","metadata":{}},{"cell_type":"markdown","source":"Optional Streamlit model toggle to choose model user prefers (default ensemble because it's best performing)","metadata":{}},{"cell_type":"code","source":"class ClippedMultiOutputRegressor(MultiOutputRegressor):\n    def predict(self, X):\n        preds = super().predict(X)\n        return np.clip(preds, 0, 1)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-08T10:59:30.152351Z","iopub.execute_input":"2025-06-08T10:59:30.152689Z","iopub.status.idle":"2025-06-08T10:59:30.171153Z","shell.execute_reply.started":"2025-06-08T10:59:30.152632Z","shell.execute_reply":"2025-06-08T10:59:30.170013Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Ridge Regression","metadata":{}},{"cell_type":"code","source":"# Initialize model\nridge = Ridge(alpha=1.0, solver='lsqr', random_state=42)\nridge_reg = ClippedMultiOutputRegressor(ridge)\n\nridge_reg.fit(X_train, y_train_reg)\n\ny_pred_ridge = ridge_reg.predict(X_test)\n\nfor i, emotion in enumerate(emotion_cols):\n    print(f\"\\nEmotion: {emotion}\")\n    print(f\"  MAE: {mean_absolute_error(y_test_reg[:, i], y_pred_ridge[:, i]):.4f}\")\n    print(f\"  MSE: {mean_squared_error(y_test_reg[:, i], y_pred_ridge[:, i]):.4f}\")\n    print(f\"  R^2: {r2_score(y_test_reg[:, i], y_pred_ridge[:, i]):.4f}\")\n    corr, _ = pearsonr(y_test_reg[:, i], y_pred_ridge[:, i])\n    print(f\"  Pearson: {corr:.4f}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-08T10:59:30.172329Z","iopub.execute_input":"2025-06-08T10:59:30.172707Z","iopub.status.idle":"2025-06-08T10:59:30.488367Z","shell.execute_reply.started":"2025-06-08T10:59:30.172675Z","shell.execute_reply":"2025-06-08T10:59:30.487461Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## LightGBM Regressor","metadata":{}},{"cell_type":"code","source":"from lightgbm import LGBMRegressor\nlgbm_base = LGBMRegressor(\n    num_leaves=20,\n    n_estimators=500,\n    learning_rate=0.1,\n    reg_alpha=0.1,\n    min_child_samples=3,\n    colsample_bytree=0.3,\n    random_state=42,\n    n_jobs=1\n)\n\nlgbm_reg = ClippedMultiOutputRegressor(lgbm_base)\n\nlgbm_reg.fit(X_train, y_train_reg)\n\ny_pred_lgb = lgbm_reg.predict(X_test)\n\nfor i, emotion in enumerate(emotion_cols):\n    print(f\"\\nEmotion: {emotion}\")\n    print(f\"  MAE: {mean_absolute_error(y_test_reg[:, i], y_pred_lgb[:, i]):.4f}\")\n    print(f\"  MSE: {mean_squared_error(y_test_reg[:, i], y_pred_lgb[:, i]):.4f}\")\n    print(f\"  R^2: {r2_score(y_test_reg[:, i], y_pred_lgb[:, i]):.4f}\")\n    corr, _ = pearsonr(y_test_reg[:, i], y_pred_lgb[:, i])\n    print(f\"  Pearson: {corr:.4f}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-08T10:59:30.489567Z","iopub.execute_input":"2025-06-08T10:59:30.489938Z","iopub.status.idle":"2025-06-08T10:59:51.355931Z","shell.execute_reply.started":"2025-06-08T10:59:30.489910Z","shell.execute_reply":"2025-06-08T10:59:51.354845Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Ensemble (Ridge & LightGBM)","metadata":{}},{"cell_type":"code","source":"from sklearn.base import BaseEstimator, RegressorMixin\n\nclass EnsembleRegressor(BaseEstimator, RegressorMixin):\n    def __init__(self, model1, model2, alpha=0.3):\n        self.model1 = model1\n        self.model2 = model2\n        self.alpha = alpha\n\n    def fit(self, X, y):\n        self.model1.fit(X, y)\n        self.model2.fit(X, y)\n        return self\n\n    def predict(self, X):\n        pred1 = self.model1.predict(X)\n        pred2 = self.model2.predict(X)\n        return self.alpha * pred1 + (1 - self.alpha) * pred2\n\n# Optional to set alpha in streamlit (default alpha = 0.3)\nalpha = 0.3\nensemble_reg = EnsembleRegressor(model1=ridge_reg, model2=lgbm_reg, alpha=alpha)\nensemble_reg.fit(X_train, y_train_reg)\ny_pred_ensemble = ensemble_reg.predict(X_test)\n\nfor i, emotion in enumerate(emotion_cols):\n    print(f\"\\nEmotion: {emotion}\")\n    print(f\"  MAE: {mean_absolute_error(y_test_reg[:, i], y_pred_ensemble[:, i]):.4f}\")\n    print(f\"  MSE: {mean_squared_error(y_test_reg[:, i], y_pred_ensemble[:, i]):.4f}\")\n    print(f\"  R^2: {r2_score(y_test_reg[:, i], y_pred_ensemble[:, i]):.4f}\")\n    corr, _ = pearsonr(y_test_reg[:, i], y_pred_ensemble[:, i])\n    print(f\"  Pearson: {corr:.4f}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-08T10:59:51.356870Z","iopub.execute_input":"2025-06-08T10:59:51.357132Z","iopub.status.idle":"2025-06-08T11:00:13.401724Z","shell.execute_reply.started":"2025-06-08T10:59:51.357095Z","shell.execute_reply":"2025-06-08T11:00:13.400918Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Regression Evaluation Plot","metadata":{}},{"cell_type":"code","source":"y_true_flat = y_test_reg.flatten()\ny_pred_flat = y_pred_ensemble.flatten() #Sesuaikan dengan y_pred model","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-08T11:00:13.402396Z","iopub.execute_input":"2025-06-08T11:00:13.402730Z","iopub.status.idle":"2025-06-08T11:00:13.408501Z","shell.execute_reply.started":"2025-06-08T11:00:13.402706Z","shell.execute_reply":"2025-06-08T11:00:13.407192Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"fig, axes = plt.subplots(2, 1, figsize=(10, 6), sharex=True, sharey=True)\n\n# True Intensities\naxes[0].plot(y_true_flat, color=\"tab:blue\", linewidth=1.5)\naxes[0].set_title(\"True Emotion Intensities\")\n\n# Predicted Intensities\naxes[1].plot(y_pred_flat, color=\"tab:orange\", linewidth=1.5)\naxes[1].set_title(\"Predicted Emotion Intensities\")\n\nfig.text(0.5, 0.04, 'Flattened Sample Index', ha='center', fontsize=12)\nfig.text(0.04, 0.5, 'Emotion Intensity', va='center', rotation='vertical', fontsize=12)\n\nplt.tight_layout(rect=[0.05, 0.05, 1, 1])\nplt.show()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-08T11:00:13.409433Z","iopub.execute_input":"2025-06-08T11:00:13.409776Z","iopub.status.idle":"2025-06-08T11:00:14.192682Z","shell.execute_reply.started":"2025-06-08T11:00:13.409743Z","shell.execute_reply":"2025-06-08T11:00:14.191737Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Classification Model","metadata":{}},{"cell_type":"markdown","source":"4 Optional models that can be toggled in Streamlit. Default Ensemble LR + LGBM because it got the best results.","metadata":{}},{"cell_type":"markdown","source":"## Logistic Regression","metadata":{}},{"cell_type":"code","source":"logR = LogisticRegression(max_iter=50, solver='newton-cg', random_state=42)\nlogR.fit(X_train, y_train_cls)\n\ny_pred_logR = logR.predict(X_test)\n\nprint(\"Accuracy:\", accuracy_score(y_test_cls, y_pred_logR))\nprint(classification_report(y_test_cls, y_pred_logR, target_names=emotion_cols))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-08T11:00:14.196209Z","iopub.execute_input":"2025-06-08T11:00:14.196509Z","iopub.status.idle":"2025-06-08T11:00:15.499253Z","shell.execute_reply.started":"2025-06-08T11:00:14.196486Z","shell.execute_reply":"2025-06-08T11:00:15.498175Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## LightGBM Classifier","metadata":{}},{"cell_type":"code","source":"lgbm_clf = LGBMClassifier(\n    num_leaves=20,\n    n_estimators=500,\n    learning_rate=0.1,\n    reg_alpha=0.1,\n    min_child_samples=3,\n    colsample_bytree=0.5,\n    random_state=42,\n    n_jobs=1\n)\n\nlgbm_clf.fit(X_train, y_train_cls)\ny_pred_lgb_clf = lgbm_clf.predict(X_test)\n\n\nprint(\"Accuracy:\", accuracy_score(y_test_cls, y_pred_lgb_clf))\nprint(classification_report(y_test_cls, y_pred_lgb_clf, target_names=emotion_cols))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-08T11:00:15.500282Z","iopub.execute_input":"2025-06-08T11:00:15.500566Z","iopub.status.idle":"2025-06-08T11:00:42.033465Z","shell.execute_reply.started":"2025-06-08T11:00:15.500537Z","shell.execute_reply":"2025-06-08T11:00:42.032321Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Support Vector Machines (SVM)","metadata":{}},{"cell_type":"code","source":"from sklearn.svm import SVC\nfrom sklearn.multioutput import MultiOutputClassifier\n\nsvm_model = SVC(kernel='linear', C=1.0, probability=True, max_iter=10000, random_state=42)\nsvm_model.fit(X_train, y_train_cls)\ny_pred_svm = svm_model.predict(X_test)\n\nprint(\"Accuracy:\", accuracy_score(y_test_cls, y_pred_svm))\nprint(classification_report(y_test_cls, y_pred_svm, target_names=emotion_cols))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-08T11:00:42.034571Z","iopub.execute_input":"2025-06-08T11:00:42.034873Z","iopub.status.idle":"2025-06-08T11:01:46.202672Z","shell.execute_reply.started":"2025-06-08T11:00:42.034850Z","shell.execute_reply":"2025-06-08T11:01:46.201717Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Ensemble Logistic Regression & LightGBM (BEST)","metadata":{}},{"cell_type":"code","source":"from sklearn.ensemble import VotingClassifier\n\nensemble_clf = VotingClassifier(\n    estimators=[\n        ('lr', logR),\n        ('lgbm', lgbm_clf),\n    ],\n    voting='soft'\n)\nensemble_clf.fit(X_train, y_train_cls)\ny_pred_ensemble = ensemble_clf.predict(X_test)\n\nprint(\"Accuracy:\", accuracy_score(y_test_cls, y_pred_ensemble))\nprint(classification_report(y_test_cls, y_pred_ensemble, target_names=emotion_cols))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-08T11:01:46.203935Z","iopub.execute_input":"2025-06-08T11:01:46.204283Z","iopub.status.idle":"2025-06-08T11:02:14.695491Z","shell.execute_reply.started":"2025-06-08T11:01:46.204260Z","shell.execute_reply":"2025-06-08T11:02:14.694269Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Classification Evaluation Plots","metadata":{}},{"cell_type":"code","source":"# Sesuaikan y_pred masing2 model\ncm = confusion_matrix(y_test_cls, y_pred_ensemble)\n\nplt.figure(figsize=(6, 5))\nsns.heatmap(cm, annot=True, fmt='d', cmap='Blues', xticklabels=emotion_cols, yticklabels=emotion_cols)\nplt.xlabel(\"Predicted Label\")\nplt.ylabel(\"True Label\")\nplt.title(\"Confusion Matrix\")\nplt.tight_layout()\nplt.show()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-08T11:02:14.697150Z","iopub.execute_input":"2025-06-08T11:02:14.697819Z","iopub.status.idle":"2025-06-08T11:02:15.015150Z","shell.execute_reply.started":"2025-06-08T11:02:14.697789Z","shell.execute_reply":"2025-06-08T11:02:15.014247Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Final Model Pipeline","metadata":{}},{"cell_type":"code","source":"def EmoIntPipeline(texts):\n    if isinstance(texts, str):\n        texts = [texts]\n\n    # Clean text and extract tfidf\n    texts = [clean_text(t) for t in texts]\n    tfidf_feat = tfidf.transform(texts)\n\n    # Extract lexicons\n    lex_feat = np.array([extract_lex(t, nrc_lexicon) for t in texts])\n    vad_feat = np.array([extract_vad(t, nrc_vad_lexicon) for t in texts])\n    hash_feat = np.array([extract_hash_emo(t, hash_emo_lex) for t in texts])\n\n    # Scale lexicons\n    lex_feat = scaler_lex.transform(lex_feat)\n    vad_feat = scaler_vad.transform(vad_feat)\n    hash_feat = scaler_hash.transform(hash_feat)\n\n    # Combine lexicons\n    combined_lex = np.concatenate([vad_feat, lex_feat, hash_feat], axis=1)\n    combined_feat = hstack([tfidf_feat, combined_lex])\n\n    # Classification\n    probs = ensemble_clf.predict_proba(combined_feat)\n    confidence_scores = np.max(probs, axis=1)\n    predicted_classes = np.argmax(probs, axis=1)\n    pred_emotion = [emotion_cols[i] for i in predicted_classes]\n\n    # Regression\n    pred_reg_all = ensemble_reg.predict(combined_feat)\n    pred_intensity = [pred[i] for pred, i in zip(pred_reg_all, predicted_classes)]\n\n    rets = []\n    for text, em, score, conf in zip(texts, pred_emotion, pred_intensity, confidence_scores):\n        if conf > 0.4 and score < 0.4:\n            boost = 0.3 * conf\n            score = min(score + boost, 1.0)\n        rets.append((text, em, score, conf))\n\n    return rets","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-08T11:02:15.016100Z","iopub.execute_input":"2025-06-08T11:02:15.016343Z","iopub.status.idle":"2025-06-08T11:02:15.027008Z","shell.execute_reply.started":"2025-06-08T11:02:15.016325Z","shell.execute_reply":"2025-06-08T11:02:15.025940Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"sample_text = [\n    \"I'm so happy and grateful today!\",\n    \"I'm kinda happy today, but at the same time life is so bland\",\n    \"I might be happy today, but it's just a normal day.\",\n    \"This makes me so angry, I can't believe they did that.\",\n    \"I'm angry, but I think I can tolerate their behavior.\",\n    \"I'm extremely furious, he never get things right.\",\n    \"I'm feeling a bit down today, things aren't going as planned.\",\n    \"My girlfriend just dumped me, I don't know what to do with my life anymore.\",\n    \"I'm crying my eyes out now, a family member of mine just passed away.\",\n    \"That movie was terrifying, I couldn't sleep all night.\",\n    \"The haunted house was scary, but we had so much fun\",\n    \"That scared me so much, I almost had a heart attack.\"\n]\nrets = EmoIntPipeline(sample_text)\nfor text, emotion, intensity, confidence in rets:\n    print(f\"Text: {text}\")\n    print(f\"Emotion: {emotion}\")\n    print(f\"Intensity: {intensity:.3f}\")\n    # print(f\"Probability: {confidence:.3f}\")\n    print(\"-\" * 40)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-08T11:02:15.028262Z","iopub.execute_input":"2025-06-08T11:02:15.028635Z","iopub.status.idle":"2025-06-08T11:02:15.111720Z","shell.execute_reply.started":"2025-06-08T11:02:15.028612Z","shell.execute_reply":"2025-06-08T11:02:15.109343Z"}},"outputs":[],"execution_count":null}]}