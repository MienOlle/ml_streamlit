{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b390ca25",
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2025-06-02T10:08:34.902150Z",
     "iopub.status.busy": "2025-06-02T10:08:34.901816Z",
     "iopub.status.idle": "2025-06-02T10:08:44.828559Z",
     "shell.execute_reply": "2025-06-02T10:08:44.827063Z"
    },
    "papermill": {
     "duration": 9.937824,
     "end_time": "2025-06-02T10:08:44.831546",
     "exception": false,
     "start_time": "2025-06-02T10:08:34.893722",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting emoji\n",
      "  Downloading emoji-2.14.1-py3-none-any.whl.metadata (5.7 kB)\n",
      "Collecting contractions\n",
      "  Downloading contractions-0.1.73-py2.py3-none-any.whl.metadata (1.2 kB)\n",
      "Collecting textsearch>=0.0.21 (from contractions)\n",
      "  Downloading textsearch-0.0.24-py2.py3-none-any.whl.metadata (1.2 kB)\n",
      "Collecting anyascii (from textsearch>=0.0.21->contractions)\n",
      "  Downloading anyascii-0.3.2-py3-none-any.whl.metadata (1.5 kB)\n",
      "Collecting pyahocorasick (from textsearch>=0.0.21->contractions)\n",
      "  Downloading pyahocorasick-2.1.0-cp312-cp312-win_amd64.whl.metadata (13 kB)\n",
      "Downloading emoji-2.14.1-py3-none-any.whl (590 kB)\n",
      "   ---------------------------------------- 0.0/590.6 kB ? eta -:--:--\n",
      "   ---------------------------------------- 590.6/590.6 kB 4.2 MB/s eta 0:00:00\n",
      "Downloading contractions-0.1.73-py2.py3-none-any.whl (8.7 kB)\n",
      "Downloading textsearch-0.0.24-py2.py3-none-any.whl (7.6 kB)\n",
      "Downloading anyascii-0.3.2-py3-none-any.whl (289 kB)\n",
      "Downloading pyahocorasick-2.1.0-cp312-cp312-win_amd64.whl (39 kB)\n",
      "Installing collected packages: pyahocorasick, emoji, anyascii, textsearch, contractions\n",
      "Successfully installed anyascii-0.3.2 contractions-0.1.73 emoji-2.14.1 pyahocorasick-2.1.0 textsearch-0.0.24\n",
      "Collecting lightgbm\n",
      "  Downloading lightgbm-4.6.0-py3-none-win_amd64.whl.metadata (17 kB)\n",
      "Requirement already satisfied: numpy>=1.17.0 in d:\\miniconda\\lib\\site-packages (from lightgbm) (2.2.0)\n",
      "Requirement already satisfied: scipy in d:\\miniconda\\lib\\site-packages (from lightgbm) (1.14.1)\n",
      "Downloading lightgbm-4.6.0-py3-none-win_amd64.whl (1.5 MB)\n",
      "   ---------------------------------------- 0.0/1.5 MB ? eta -:--:--\n",
      "   -------------- ------------------------- 0.5/1.5 MB 4.2 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 1.5/1.5 MB 4.0 MB/s eta 0:00:00\n",
      "Installing collected packages: lightgbm\n",
      "Successfully installed lightgbm-4.6.0\n"
     ]
    }
   ],
   "source": [
    "!pip install emoji contractions\n",
    "!pip install lightgbm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ea5ddee",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-02T10:08:44.847777Z",
     "iopub.status.busy": "2025-06-02T10:08:44.847401Z",
     "iopub.status.idle": "2025-06-02T10:08:55.913171Z",
     "shell.execute_reply": "2025-06-02T10:08:55.912086Z"
    },
    "papermill": {
     "duration": 11.075787,
     "end_time": "2025-06-02T10:08:55.915065",
     "exception": false,
     "start_time": "2025-06-02T10:08:44.839278",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import joblib\n",
    "import re\n",
    "import emoji\n",
    "import contractions\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import random\n",
    "from sklearn.metrics import classification_report, accuracy_score, f1_score\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.multioutput import MultiOutputRegressor\n",
    "from scipy.sparse import hstack\n",
    "from scipy.stats import pearsonr\n",
    "from collections import defaultdict\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression, Ridge\n",
    "from sklearn.multiclass import OneVsRestClassifier\n",
    "from lightgbm import LGBMRegressor\n",
    "from lightgbm import LGBMClassifier"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d90241c2",
   "metadata": {
    "papermill": {
     "duration": 0.007634,
     "end_time": "2025-06-02T10:08:55.930134",
     "exception": false,
     "start_time": "2025-06-02T10:08:55.922500",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Data Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "5527be0a",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-02T10:08:55.946973Z",
     "iopub.status.busy": "2025-06-02T10:08:55.946241Z",
     "iopub.status.idle": "2025-06-02T10:08:56.061065Z",
     "shell.execute_reply": "2025-06-02T10:08:56.059181Z"
    },
    "papermill": {
     "duration": 0.126547,
     "end_time": "2025-06-02T10:08:56.064178",
     "exception": false,
     "start_time": "2025-06-02T10:08:55.937631",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set: (8872, 6)\n",
      "Test set: (2218, 6)\n"
     ]
    }
   ],
   "source": [
    "# Load datasets\n",
    "train = pd.read_csv('train.csv', delimiter='\\t')\n",
    "test = pd.read_csv('test.csv', delimiter='\\t')\n",
    "dev = pd.read_csv('dev.csv', delimiter='\\t')\n",
    "\n",
    "combined_df = pd.concat([train, dev, test], ignore_index=True)\n",
    "\n",
    "# Atur parameter test_size di Streamlit\n",
    "train_df, test_df = train_test_split(combined_df, test_size=0.2, random_state=42)\n",
    "\n",
    "print(f\"Training set: {train_df.shape}\")\n",
    "print(f\"Test set: {test_df.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f77e28d",
   "metadata": {
    "papermill": {
     "duration": 0.007434,
     "end_time": "2025-06-02T10:08:56.080041",
     "exception": false,
     "start_time": "2025-06-02T10:08:56.072607",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Data Cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "98ce5af7",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-02T10:08:56.098894Z",
     "iopub.status.busy": "2025-06-02T10:08:56.098546Z",
     "iopub.status.idle": "2025-06-02T10:08:57.500896Z",
     "shell.execute_reply": "2025-06-02T10:08:57.499421Z"
    },
    "papermill": {
     "duration": 1.415038,
     "end_time": "2025-06-02T10:08:57.503037",
     "exception": false,
     "start_time": "2025-06-02T10:08:56.087999",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Text preprocessing functions\n",
    "def convert_emojis(text):\n",
    "    text = emoji.demojize(text, delimiters=(\" \", \" \"))\n",
    "    text = re.sub(r':([a-zA-Z_]+):', r'\\1', text)\n",
    "    text = re.sub(r'\\s+', ' ', text).strip()\n",
    "    return text\n",
    "\n",
    "def clean_text(text):\n",
    "    # Lowercase\n",
    "    text = text.lower()\n",
    "    # expand contractions\n",
    "    text = contractions.fix(text)\n",
    "    # convert emojis\n",
    "    text = convert_emojis(text)\n",
    "    # Remove URLs\n",
    "    text = re.sub(r\"http\\S+|www\\S+|https\\S+\", '', text, flags=re.MULTILINE)\n",
    "    # Remove user mentions and hashtags\n",
    "    # text = re.sub(r'\\@\\w+|\\#','', text)\n",
    "    text = re.sub(r'@\\w+', '', text)\n",
    "    # Remove special characters and numbers (except punctuation)\n",
    "    text = re.sub(r\"[^a-zA-Z\\s.,!?']\", '', text)\n",
    "    # Remove extra spaces\n",
    "    text = re.sub(r'\\s+', ' ', text).strip()\n",
    "    return text\n",
    "\n",
    "# Apply text cleaning\n",
    "train_df[\"clean_text\"] = train_df[\"Tweet\"].apply(clean_text)\n",
    "test_df[\"clean_text\"] = test_df[\"Tweet\"].apply(clean_text)\n",
    "\n",
    "# Define emotion columns\n",
    "emotion_cols = [\"joy\", \"sadness\", \"anger\", \"fear\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "98e0dcdf",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-02T10:08:57.523951Z",
     "iopub.status.busy": "2025-06-02T10:08:57.523398Z",
     "iopub.status.idle": "2025-06-02T10:08:57.539472Z",
     "shell.execute_reply": "2025-06-02T10:08:57.538432Z"
    },
    "papermill": {
     "duration": 0.03077,
     "end_time": "2025-06-02T10:08:57.541919",
     "exception": false,
     "start_time": "2025-06-02T10:08:57.511149",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Missing values in emotion columns:\n",
      "\n",
      "train dataset:\n",
      "joy: 0 missing values (0.0%)\n",
      "sadness: 0 missing values (0.0%)\n",
      "anger: 0 missing values (0.0%)\n",
      "fear: 0 missing values (0.0%)\n",
      "\n",
      "test dataset:\n",
      "joy: 0 missing values (0.0%)\n",
      "sadness: 0 missing values (0.0%)\n",
      "anger: 0 missing values (0.0%)\n",
      "fear: 0 missing values (0.0%)\n"
     ]
    }
   ],
   "source": [
    "# Check for missing values in emotion columns\n",
    "print(\"Missing values in emotion columns:\")\n",
    "for df, name in [(train_df, \"train\"), (test_df, \"test\")]:\n",
    "    print(f\"\\n{name} dataset:\")\n",
    "    for col in emotion_cols:\n",
    "        missing = df[col].isna().sum()\n",
    "        total = len(df)\n",
    "        print(f\"{col}: {missing} missing values ({missing/total*100:.1f}%)\")\n",
    "\n",
    "# Fill missing values with 0 (indicating absence of that emotion)\n",
    "for df in [train_df, test_df]:\n",
    "    for col in emotion_cols:\n",
    "        df[col] = df[col].fillna(0.0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b0d09ea",
   "metadata": {
    "papermill": {
     "duration": 0.012185,
     "end_time": "2025-06-02T10:08:57.567579",
     "exception": false,
     "start_time": "2025-06-02T10:08:57.555394",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Lexicons"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "666e93b6",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-02T10:08:57.590600Z",
     "iopub.status.busy": "2025-06-02T10:08:57.590120Z",
     "iopub.status.idle": "2025-06-02T10:08:57.775034Z",
     "shell.execute_reply": "2025-06-02T10:08:57.773507Z"
    },
    "papermill": {
     "duration": 0.19822,
     "end_time": "2025-06-02T10:08:57.777646",
     "exception": false,
     "start_time": "2025-06-02T10:08:57.579426",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'NRC-Emotion-Lexicon-Wordlevel-v0.92.txt'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mFileNotFoundError\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[10]\u001b[39m\u001b[32m, line 11\u001b[39m\n\u001b[32m      8\u001b[39m                 lexicon[word][emotion] = \u001b[32m1\u001b[39m\n\u001b[32m      9\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m lexicon\n\u001b[32m---> \u001b[39m\u001b[32m11\u001b[39m nrc_lexicon = \u001b[43mload_lex\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mNRC-Emotion-Lexicon-Wordlevel-v0.92.txt\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m     13\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mextract_lex\u001b[39m(text, lexicon):\n\u001b[32m     14\u001b[39m     emotions = [\u001b[33m'\u001b[39m\u001b[33manger\u001b[39m\u001b[33m'\u001b[39m, \u001b[33m'\u001b[39m\u001b[33manticipation\u001b[39m\u001b[33m'\u001b[39m, \u001b[33m'\u001b[39m\u001b[33mdisgust\u001b[39m\u001b[33m'\u001b[39m, \u001b[33m'\u001b[39m\u001b[33mfear\u001b[39m\u001b[33m'\u001b[39m, \u001b[33m'\u001b[39m\u001b[33mjoy\u001b[39m\u001b[33m'\u001b[39m,\n\u001b[32m     15\u001b[39m               \u001b[33m'\u001b[39m\u001b[33msadness\u001b[39m\u001b[33m'\u001b[39m, \u001b[33m'\u001b[39m\u001b[33msurprise\u001b[39m\u001b[33m'\u001b[39m, \u001b[33m'\u001b[39m\u001b[33mtrust\u001b[39m\u001b[33m'\u001b[39m, \u001b[33m'\u001b[39m\u001b[33mpositive\u001b[39m\u001b[33m'\u001b[39m, \u001b[33m'\u001b[39m\u001b[33mnegative\u001b[39m\u001b[33m'\u001b[39m]\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[10]\u001b[39m\u001b[32m, line 4\u001b[39m, in \u001b[36mload_lex\u001b[39m\u001b[34m(filepath)\u001b[39m\n\u001b[32m      2\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mload_lex\u001b[39m(filepath):\n\u001b[32m      3\u001b[39m     lexicon = defaultdict(\u001b[38;5;28mdict\u001b[39m)\n\u001b[32m----> \u001b[39m\u001b[32m4\u001b[39m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mfilepath\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mr\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mas\u001b[39;00m file:\n\u001b[32m      5\u001b[39m         \u001b[38;5;28;01mfor\u001b[39;00m line \u001b[38;5;129;01min\u001b[39;00m file:\n\u001b[32m      6\u001b[39m             word, emotion, value = line.strip().split(\u001b[33m'\u001b[39m\u001b[38;5;130;01m\\t\u001b[39;00m\u001b[33m'\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Roaming\\Python\\Python312\\site-packages\\IPython\\core\\interactiveshell.py:325\u001b[39m, in \u001b[36m_modified_open\u001b[39m\u001b[34m(file, *args, **kwargs)\u001b[39m\n\u001b[32m    318\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m file \u001b[38;5;129;01min\u001b[39;00m {\u001b[32m0\u001b[39m, \u001b[32m1\u001b[39m, \u001b[32m2\u001b[39m}:\n\u001b[32m    319\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[32m    320\u001b[39m         \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mIPython won\u001b[39m\u001b[33m'\u001b[39m\u001b[33mt let you open fd=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfile\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m by default \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    321\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mas it is likely to crash IPython. If you know what you are doing, \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    322\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33myou can use builtins\u001b[39m\u001b[33m'\u001b[39m\u001b[33m open.\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    323\u001b[39m     )\n\u001b[32m--> \u001b[39m\u001b[32m325\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mio_open\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfile\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[31mFileNotFoundError\u001b[39m: [Errno 2] No such file or directory: 'NRC-Emotion-Lexicon-Wordlevel-v0.92.txt'"
     ]
    }
   ],
   "source": [
    "# Load EmoLex features\n",
    "def load_lex(filepath):\n",
    "    lexicon = defaultdict(dict)\n",
    "    with open(filepath, 'r') as file:\n",
    "        for line in file:\n",
    "            word, emotion, value = line.strip().split('\\t')\n",
    "            if int(value) == 1:\n",
    "                lexicon[word][emotion] = 1\n",
    "    return lexicon\n",
    "\n",
    "nrc_lexicon = load_lex(\"/kaggle/input/nrc-lexicons/NRC-Emotion-Lexicon-Wordlevel-v0.92.txt\")\n",
    "\n",
    "def extract_lex(text, lexicon):\n",
    "    emotions = ['anger', 'anticipation', 'disgust', 'fear', 'joy',\n",
    "              'sadness', 'surprise', 'trust', 'positive', 'negative']\n",
    "    counts = dict.fromkeys(emotions, 0)\n",
    "\n",
    "    for word in text.split():\n",
    "        if word in lexicon:\n",
    "            for emo in lexicon[word]:\n",
    "                counts[emo] += 1\n",
    "    return [counts[emo] for emo in emotions]\n",
    "\n",
    "# Extract lexicon features\n",
    "train_df['lexicons'] = train_df['clean_text'].apply(lambda x: extract_lex(x, nrc_lexicon))\n",
    "test_df['lexicons'] = test_df['clean_text'].apply(lambda x: extract_lex(x, nrc_lexicon))\n",
    "\n",
    "train_lex = np.array(train_df['lexicons'].tolist())\n",
    "test_lex = np.array(test_df['lexicons'].tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2812a15c",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-02T10:08:57.794568Z",
     "iopub.status.busy": "2025-06-02T10:08:57.794193Z",
     "iopub.status.idle": "2025-06-02T10:08:58.308839Z",
     "shell.execute_reply": "2025-06-02T10:08:58.307300Z"
    },
    "papermill": {
     "duration": 0.525151,
     "end_time": "2025-06-02T10:08:58.310853",
     "exception": false,
     "start_time": "2025-06-02T10:08:57.785702",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Load VAD Lexicons\n",
    "def load_nrc_vad(filepath):\n",
    "    vad_lex = {}\n",
    "    with open(filepath, 'r', encoding='utf-8') as f:\n",
    "        next(f)  # skip header\n",
    "        for line in f:\n",
    "            word, val, aro, dom = line.strip().split('\\t')\n",
    "            vad_lex[word] = {\n",
    "                'valence': float(val),\n",
    "                'arousal': float(aro),\n",
    "                'dominance': float(dom)\n",
    "            }\n",
    "    return vad_lex\n",
    "\n",
    "nrc_vad_lexicon = load_nrc_vad(\"NRC-VAD-Lexicon-v2.1.txt\")\n",
    "\n",
    "def extract_vad(text, lexicon):\n",
    "    valence = []\n",
    "    arousal = []\n",
    "    dominance = []\n",
    "\n",
    "    for word in text.split():\n",
    "        if word in lexicon:\n",
    "            valence.append(lexicon[word]['valence'])\n",
    "            arousal.append(lexicon[word]['arousal'])\n",
    "            dominance.append(lexicon[word]['dominance'])\n",
    "\n",
    "    # If no word matched, return zeros\n",
    "    if not valence:\n",
    "        return [0.0, 0.0, 0.0]\n",
    "\n",
    "    # Otherwise, return means\n",
    "    return [\n",
    "        np.mean(valence),\n",
    "        np.mean(arousal),\n",
    "        np.mean(dominance)\n",
    "    ]\n",
    "\n",
    "# Extract lexicon features\n",
    "train_df['vad'] = train_df['clean_text'].apply(lambda x: extract_vad(x, nrc_vad_lexicon))\n",
    "test_df['vad'] = test_df['clean_text'].apply(lambda x: extract_vad(x, nrc_vad_lexicon))\n",
    "\n",
    "train_vad = np.array(train_df['vad'].tolist())\n",
    "test_vad = np.array(test_df['vad'].tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a6085f4",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-02T10:08:58.326964Z",
     "iopub.status.busy": "2025-06-02T10:08:58.326626Z",
     "iopub.status.idle": "2025-06-02T10:08:59.148292Z",
     "shell.execute_reply": "2025-06-02T10:08:59.146492Z"
    },
    "papermill": {
     "duration": 0.831864,
     "end_time": "2025-06-02T10:08:59.150082",
     "exception": false,
     "start_time": "2025-06-02T10:08:58.318218",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Load HashEmo Lexicons\n",
    "def load_nrc_hash_emo(filepath):\n",
    "    lexicon = defaultdict(dict)\n",
    "    with open(filepath, 'r', encoding='utf-8') as f:\n",
    "        for line in f:\n",
    "            emotion, word, score = line.strip().split('\\t')\n",
    "            lexicon[word][emotion] = float(score)\n",
    "    return lexicon\n",
    "\n",
    "hash_emo_lex = load_nrc_hash_emo('NRC-Hashtag-Emotion-Lexicon-v0.2.txt')\n",
    "\n",
    "def extract_hash_emo(text, lexicon):\n",
    "    emotions = ['anger', 'anticipation', 'disgust', 'fear', 'joy',\n",
    "                'sadness', 'surprise', 'trust']\n",
    "    scores = {emo: [] for emo in emotions}\n",
    "\n",
    "    for word in text.split():\n",
    "        if word in lexicon:\n",
    "            for emo, value in lexicon[word].items():\n",
    "                scores[emo].append(value)\n",
    "\n",
    "    return [np.mean(scores[emo]) if scores[emo] else 0.0 for emo in emotions]\n",
    "\n",
    "train_df['hash'] = train_df['clean_text'].apply(lambda x: extract_hash_emo(x, hash_emo_lex))\n",
    "test_df['hash'] = test_df['clean_text'].apply(lambda x: extract_hash_emo(x, hash_emo_lex))\n",
    "\n",
    "train_hash = np.array(train_df['hash'].tolist())\n",
    "test_hash = np.array(test_df['hash'].tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "28b56645",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-02T10:08:59.167618Z",
     "iopub.status.busy": "2025-06-02T10:08:59.167271Z",
     "iopub.status.idle": "2025-06-02T10:08:59.201254Z",
     "shell.execute_reply": "2025-06-02T10:08:59.199724Z"
    },
    "papermill": {
     "duration": 0.04519,
     "end_time": "2025-06-02T10:08:59.203656",
     "exception": false,
     "start_time": "2025-06-02T10:08:59.158466",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "scaler_hash = StandardScaler()\n",
    "train_hash = scaler_hash.fit_transform(train_hash)\n",
    "test_hash = scaler_hash.transform(test_hash)\n",
    "\n",
    "scaler_lex = StandardScaler()\n",
    "train_lex = scaler_lex.fit_transform(train_lex)\n",
    "test_lex = scaler_lex.transform(test_lex)\n",
    "\n",
    "scaler_vad = StandardScaler()\n",
    "train_vad = scaler_vad.fit_transform(train_vad)\n",
    "test_vad = scaler_vad.transform(test_vad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "52f0a729",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-02T10:08:59.220550Z",
     "iopub.status.busy": "2025-06-02T10:08:59.220223Z",
     "iopub.status.idle": "2025-06-02T10:08:59.229034Z",
     "shell.execute_reply": "2025-06-02T10:08:59.227448Z"
    },
    "papermill": {
     "duration": 0.019386,
     "end_time": "2025-06-02T10:08:59.231322",
     "exception": false,
     "start_time": "2025-06-02T10:08:59.211936",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# NRC Hash-Emo + EmoLex + VAD\n",
    "train_combined = np.concatenate([train_vad, train_lex, train_hash], axis=1)\n",
    "test_combined = np.concatenate([test_vad, test_lex, test_hash], axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aca7a1cb",
   "metadata": {
    "papermill": {
     "duration": 0.006874,
     "end_time": "2025-06-02T10:08:59.245681",
     "exception": false,
     "start_time": "2025-06-02T10:08:59.238807",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Extract Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "70b292c3",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-02T10:08:59.263169Z",
     "iopub.status.busy": "2025-06-02T10:08:59.262772Z",
     "iopub.status.idle": "2025-06-02T10:08:59.790412Z",
     "shell.execute_reply": "2025-06-02T10:08:59.789105Z"
    },
    "papermill": {
     "duration": 0.538656,
     "end_time": "2025-06-02T10:08:59.792838",
     "exception": false,
     "start_time": "2025-06-02T10:08:59.254182",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# TF-IDF Vectorization\n",
    "tfidf = TfidfVectorizer(\n",
    "    max_features=5000,\n",
    "    ngram_range=(1, 2),\n",
    "    stop_words='english'\n",
    ")\n",
    "\n",
    "# Fit on training set and transform all sets\n",
    "train_tfidf = tfidf.fit_transform(train_df['clean_text'])\n",
    "test_tfidf = tfidf.transform(test_df['clean_text'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "edf17789",
   "metadata": {
    "papermill": {
     "duration": 0.011027,
     "end_time": "2025-06-02T10:08:59.810981",
     "exception": false,
     "start_time": "2025-06-02T10:08:59.799954",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Training Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "22c5a0cb",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-02T10:08:59.828412Z",
     "iopub.status.busy": "2025-06-02T10:08:59.828027Z",
     "iopub.status.idle": "2025-06-02T10:08:59.860609Z",
     "shell.execute_reply": "2025-06-02T10:08:59.859210Z"
    },
    "papermill": {
     "duration": 0.043932,
     "end_time": "2025-06-02T10:08:59.863267",
     "exception": false,
     "start_time": "2025-06-02T10:08:59.819335",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# TF-IDF is sparse, so we use hstack (sparse-safe)\n",
    "X_train = hstack([train_tfidf, train_combined])\n",
    "X_test = hstack([test_tfidf, test_combined])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "527e5d13",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-02T10:08:59.885065Z",
     "iopub.status.busy": "2025-06-02T10:08:59.884616Z",
     "iopub.status.idle": "2025-06-02T10:08:59.903878Z",
     "shell.execute_reply": "2025-06-02T10:08:59.902293Z"
    },
    "papermill": {
     "duration": 0.030654,
     "end_time": "2025-06-02T10:08:59.906546",
     "exception": false,
     "start_time": "2025-06-02T10:08:59.875892",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# y_train_cls = (train_df[emotion_cols] > 0).astype(int).values\n",
    "# y_test_cls  = (test_df[emotion_cols] > 0).astype(int).values\n",
    "\n",
    "y_train_cls = train_df[emotion_cols].values.argmax(axis=1)\n",
    "y_test_cls = test_df[emotion_cols].values.argmax(axis=1)\n",
    "\n",
    "label_map = {0: 'joy', 1: 'sadness', 2: 'anger', 3: 'fear'}\n",
    "\n",
    "y_train_reg = train_df[emotion_cols].values\n",
    "y_test_reg = test_df[emotion_cols].values"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "960c937f",
   "metadata": {
    "papermill": {
     "duration": 0.011788,
     "end_time": "2025-06-02T10:08:59.928081",
     "exception": false,
     "start_time": "2025-06-02T10:08:59.916293",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Regression Model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7ff293d",
   "metadata": {
    "papermill": {
     "duration": 0.007671,
     "end_time": "2025-06-02T10:08:59.944761",
     "exception": false,
     "start_time": "2025-06-02T10:08:59.937090",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "Optional Streamlit model toggle to choose model user prefers (default ensemble because it's best performing)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4447e0ec",
   "metadata": {
    "papermill": {
     "duration": 0.006606,
     "end_time": "2025-06-02T10:08:59.962090",
     "exception": false,
     "start_time": "2025-06-02T10:08:59.955484",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Ridge Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "6a86b913",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-02T10:08:59.980493Z",
     "iopub.status.busy": "2025-06-02T10:08:59.980138Z",
     "iopub.status.idle": "2025-06-02T10:09:00.346925Z",
     "shell.execute_reply": "2025-06-02T10:09:00.345079Z"
    },
    "papermill": {
     "duration": 0.38019,
     "end_time": "2025-06-02T10:09:00.349148",
     "exception": false,
     "start_time": "2025-06-02T10:08:59.968958",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Emotion: joy\n",
      "  MAE: 0.1078\n",
      "  MSE: 0.0225\n",
      "  R^2: 0.6538\n",
      "  Pearson: 0.8087\n",
      "\n",
      "Emotion: sadness\n",
      "  MAE: 0.1198\n",
      "  MSE: 0.0289\n",
      "  R^2: 0.5086\n",
      "  Pearson: 0.7143\n",
      "\n",
      "Emotion: anger\n",
      "  MAE: 0.1184\n",
      "  MSE: 0.0280\n",
      "  R^2: 0.5291\n",
      "  Pearson: 0.7277\n",
      "\n",
      "Emotion: fear\n",
      "  MAE: 0.1274\n",
      "  MSE: 0.0303\n",
      "  R^2: 0.5404\n",
      "  Pearson: 0.7358\n"
     ]
    }
   ],
   "source": [
    "# Initialize model\n",
    "ridge = Ridge(alpha=1.0, solver='lsqr', random_state=42)\n",
    "ridge_reg = MultiOutputRegressor(ridge)\n",
    "\n",
    "ridge_reg.fit(X_train, y_train_reg)\n",
    "\n",
    "y_pred_ridge = ridge_reg.predict(X_test)\n",
    "\n",
    "for i, emotion in enumerate(emotion_cols):\n",
    "    print(f\"\\nEmotion: {emotion}\")\n",
    "    print(f\"  MAE: {mean_absolute_error(y_test_reg[:, i], y_pred_ridge[:, i]):.4f}\")\n",
    "    print(f\"  MSE: {mean_squared_error(y_test_reg[:, i], y_pred_ridge[:, i]):.4f}\")\n",
    "    print(f\"  R^2: {r2_score(y_test_reg[:, i], y_pred_ridge[:, i]):.4f}\")\n",
    "    corr, _ = pearsonr(y_test_reg[:, i], y_pred_ridge[:, i])\n",
    "    print(f\"  Pearson: {corr:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c69b4e2",
   "metadata": {
    "papermill": {
     "duration": 0.007105,
     "end_time": "2025-06-02T10:09:00.364222",
     "exception": false,
     "start_time": "2025-06-02T10:09:00.357117",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## LightGBM Regressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14943f4d",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-02T10:09:00.381911Z",
     "iopub.status.busy": "2025-06-02T10:09:00.381534Z",
     "iopub.status.idle": "2025-06-02T10:09:19.572736Z",
     "shell.execute_reply": "2025-06-02T10:09:19.570995Z"
    },
    "papermill": {
     "duration": 19.201671,
     "end_time": "2025-06-02T10:09:19.574786",
     "exception": false,
     "start_time": "2025-06-02T10:09:00.373115",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.191765 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 28074\n",
      "[LightGBM] [Info] Number of data points in the train set: 8872, number of used features: 4885\n",
      "[LightGBM] [Info] Start training from score 0.136371\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.191479 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 28074\n",
      "[LightGBM] [Info] Number of data points in the train set: 8872, number of used features: 4885\n",
      "[LightGBM] [Info] Start training from score 0.132116\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.211279 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 28074\n",
      "[LightGBM] [Info] Number of data points in the train set: 8872, number of used features: 4885\n",
      "[LightGBM] [Info] Start training from score 0.141451\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.187274 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 28074\n",
      "[LightGBM] [Info] Number of data points in the train set: 8872, number of used features: 4885\n",
      "[LightGBM] [Info] Start training from score 0.165232\n",
      "\n",
      "Emotion: joy\n",
      "  MAE: 0.0837\n",
      "  MSE: 0.0203\n",
      "  R^2: 0.6887\n",
      "  Pearson: 0.8300\n",
      "\n",
      "Emotion: sadness\n",
      "  MAE: 0.0992\n",
      "  MSE: 0.0263\n",
      "  R^2: 0.5528\n",
      "  Pearson: 0.7440\n",
      "\n",
      "Emotion: anger\n",
      "  MAE: 0.0996\n",
      "  MSE: 0.0261\n",
      "  R^2: 0.5615\n",
      "  Pearson: 0.7500\n",
      "\n",
      "Emotion: fear\n",
      "  MAE: 0.1131\n",
      "  MSE: 0.0285\n",
      "  R^2: 0.5688\n",
      "  Pearson: 0.7549\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.11/dist-packages/lightgbm/basic.py:1218: UserWarning: Converting data to scipy sparse matrix.\n",
      "  _log_warning(\"Converting data to scipy sparse matrix.\")\n"
     ]
    }
   ],
   "source": [
    "lgbm_base = LGBMRegressor(\n",
    "    num_leaves=20,\n",
    "    n_estimators=500,\n",
    "    learning_rate=0.1,\n",
    "    reg_alpha=0.1,\n",
    "    min_child_samples=3,\n",
    "    colsample_bytree=0.3,\n",
    "    random_state=42,\n",
    "    n_jobs=-1\n",
    ")\n",
    "\n",
    "lgbm_reg = MultiOutputRegressor(lgbm_base)\n",
    "\n",
    "lgbm_reg.fit(X_train, y_train_reg)\n",
    "\n",
    "y_pred_lgb = lgbm_reg.predict(X_test)\n",
    "\n",
    "for i, emotion in enumerate(emotion_cols):\n",
    "    print(f\"\\nEmotion: {emotion}\")\n",
    "    print(f\"  MAE: {mean_absolute_error(y_test_reg[:, i], y_pred_lgb[:, i]):.4f}\")\n",
    "    print(f\"  MSE: {mean_squared_error(y_test_reg[:, i], y_pred_lgb[:, i]):.4f}\")\n",
    "    print(f\"  R^2: {r2_score(y_test_reg[:, i], y_pred_lgb[:, i]):.4f}\")\n",
    "    corr, _ = pearsonr(y_test_reg[:, i], y_pred_lgb[:, i])\n",
    "    print(f\"  Pearson: {corr:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0da1ac3",
   "metadata": {
    "papermill": {
     "duration": 0.00729,
     "end_time": "2025-06-02T10:09:19.591998",
     "exception": false,
     "start_time": "2025-06-02T10:09:19.584708",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Ensemble (Ridge & LightGBM)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "41d01a46",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-02T10:09:19.608469Z",
     "iopub.status.busy": "2025-06-02T10:09:19.608050Z",
     "iopub.status.idle": "2025-06-02T10:09:40.570719Z",
     "shell.execute_reply": "2025-06-02T10:09:40.569233Z"
    },
    "papermill": {
     "duration": 20.973286,
     "end_time": "2025-06-02T10:09:40.572603",
     "exception": false,
     "start_time": "2025-06-02T10:09:19.599317",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.229930 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 28074\n",
      "[LightGBM] [Info] Number of data points in the train set: 8872, number of used features: 4885\n",
      "[LightGBM] [Info] Start training from score 0.136371\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.199843 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 28074\n",
      "[LightGBM] [Info] Number of data points in the train set: 8872, number of used features: 4885\n",
      "[LightGBM] [Info] Start training from score 0.132116\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.206862 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 28074\n",
      "[LightGBM] [Info] Number of data points in the train set: 8872, number of used features: 4885\n",
      "[LightGBM] [Info] Start training from score 0.141451\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.196675 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 28074\n",
      "[LightGBM] [Info] Number of data points in the train set: 8872, number of used features: 4885\n",
      "[LightGBM] [Info] Start training from score 0.165232\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.11/dist-packages/lightgbm/basic.py:1218: UserWarning: Converting data to scipy sparse matrix.\n",
      "  _log_warning(\"Converting data to scipy sparse matrix.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Emotion: joy\n",
      "  MAE: 0.0875\n",
      "  MSE: 0.0195\n",
      "  R^2: 0.7005\n",
      "  Pearson: 0.8373\n",
      "\n",
      "Emotion: sadness\n",
      "  MAE: 0.1022\n",
      "  MSE: 0.0257\n",
      "  R^2: 0.5634\n",
      "  Pearson: 0.7506\n",
      "\n",
      "Emotion: anger\n",
      "  MAE: 0.1018\n",
      "  MSE: 0.0251\n",
      "  R^2: 0.5773\n",
      "  Pearson: 0.7598\n",
      "\n",
      "Emotion: fear\n",
      "  MAE: 0.1143\n",
      "  MSE: 0.0275\n",
      "  R^2: 0.5832\n",
      "  Pearson: 0.7639\n"
     ]
    }
   ],
   "source": [
    "from sklearn.base import BaseEstimator, RegressorMixin\n",
    "\n",
    "class EnsembleRegressor(BaseEstimator, RegressorMixin):\n",
    "    def __init__(self, model1, model2, alpha=0.3):\n",
    "        self.model1 = model1\n",
    "        self.model2 = model2\n",
    "        self.alpha = alpha\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        self.model1.fit(X, y)\n",
    "        self.model2.fit(X, y)\n",
    "        return self\n",
    "\n",
    "    def predict(self, X):\n",
    "        pred1 = self.model1.predict(X)\n",
    "        pred2 = self.model2.predict(X)\n",
    "        return self.alpha * pred1 + (1 - self.alpha) * pred2\n",
    "\n",
    "# Optional to set alpha in streamlit (default alpha = 0.3)\n",
    "alpha = 0.3\n",
    "ensemble_reg = EnsembleRegressor(model1=ridge_reg, model2=lgbm_reg, alpha=alpha)\n",
    "ensemble_reg.fit(X_train, y_train_reg)\n",
    "y_pred_ensemble = ensemble_reg.predict(X_test)\n",
    "\n",
    "for i, emotion in enumerate(emotion_cols):\n",
    "    print(f\"\\nEmotion: {emotion}\")\n",
    "    print(f\"  MAE: {mean_absolute_error(y_test_reg[:, i], y_pred_ensemble[:, i]):.4f}\")\n",
    "    print(f\"  MSE: {mean_squared_error(y_test_reg[:, i], y_pred_ensemble[:, i]):.4f}\")\n",
    "    print(f\"  R^2: {r2_score(y_test_reg[:, i], y_pred_ensemble[:, i]):.4f}\")\n",
    "    corr, _ = pearsonr(y_test_reg[:, i], y_pred_ensemble[:, i])\n",
    "    print(f\"  Pearson: {corr:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04e4bdd1",
   "metadata": {
    "papermill": {
     "duration": 0.007235,
     "end_time": "2025-06-02T10:09:40.587664",
     "exception": false,
     "start_time": "2025-06-02T10:09:40.580429",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Classification Model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73cf7b2d",
   "metadata": {
    "papermill": {
     "duration": 0.00716,
     "end_time": "2025-06-02T10:09:40.602330",
     "exception": false,
     "start_time": "2025-06-02T10:09:40.595170",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "4 Optional models that can be toggled in Streamlit. Default Ensemble LR + LGBM because it got the best results."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c562ace8",
   "metadata": {
    "papermill": {
     "duration": 0.007112,
     "end_time": "2025-06-02T10:09:40.617068",
     "exception": false,
     "start_time": "2025-06-02T10:09:40.609956",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "76ba88c3",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-02T10:09:40.633833Z",
     "iopub.status.busy": "2025-06-02T10:09:40.633408Z",
     "iopub.status.idle": "2025-06-02T10:09:41.927750Z",
     "shell.execute_reply": "2025-06-02T10:09:41.926741Z"
    },
    "papermill": {
     "duration": 1.30478,
     "end_time": "2025-06-02T10:09:41.929439",
     "exception": false,
     "start_time": "2025-06-02T10:09:40.624659",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.78809738503156\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         joy       0.82      0.84      0.83       615\n",
      "     sadness       0.78      0.73      0.75       475\n",
      "       anger       0.81      0.77      0.79       526\n",
      "        fear       0.75      0.80      0.78       602\n",
      "\n",
      "    accuracy                           0.79      2218\n",
      "   macro avg       0.79      0.78      0.79      2218\n",
      "weighted avg       0.79      0.79      0.79      2218\n",
      "\n"
     ]
    }
   ],
   "source": [
    "logR = LogisticRegression(max_iter=50, solver='newton-cg', random_state=42)\n",
    "logR.fit(X_train, y_train_cls)\n",
    "\n",
    "y_pred_logR = logR.predict(X_test)\n",
    "\n",
    "print(\"Accuracy:\", accuracy_score(y_test_cls, y_pred_logR))\n",
    "print(classification_report(y_test_cls, y_pred_logR, target_names=emotion_cols))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a986a0ff",
   "metadata": {
    "papermill": {
     "duration": 0.007721,
     "end_time": "2025-06-02T10:09:41.945894",
     "exception": false,
     "start_time": "2025-06-02T10:09:41.938173",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## LightGBM Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "ea829f37",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-02T10:09:41.962385Z",
     "iopub.status.busy": "2025-06-02T10:09:41.962004Z",
     "iopub.status.idle": "2025-06-02T10:10:08.573210Z",
     "shell.execute_reply": "2025-06-02T10:10:08.571737Z"
    },
    "papermill": {
     "duration": 26.621231,
     "end_time": "2025-06-02T10:10:08.574728",
     "exception": false,
     "start_time": "2025-06-02T10:09:41.953497",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.230088 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 28074\n",
      "[LightGBM] [Info] Number of data points in the train set: 8872, number of used features: 4885\n",
      "[LightGBM] [Info] Start training from score -1.326784\n",
      "[LightGBM] [Info] Start training from score -1.548442\n",
      "[LightGBM] [Info] Start training from score -1.439535\n",
      "[LightGBM] [Info] Start training from score -1.255076\n",
      "Accuracy: 0.8183047790802525\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         joy       0.87      0.87      0.87       615\n",
      "     sadness       0.81      0.78      0.79       475\n",
      "       anger       0.85      0.79      0.82       526\n",
      "        fear       0.76      0.82      0.79       602\n",
      "\n",
      "    accuracy                           0.82      2218\n",
      "   macro avg       0.82      0.82      0.82      2218\n",
      "weighted avg       0.82      0.82      0.82      2218\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.11/dist-packages/lightgbm/basic.py:1218: UserWarning: Converting data to scipy sparse matrix.\n",
      "  _log_warning(\"Converting data to scipy sparse matrix.\")\n"
     ]
    }
   ],
   "source": [
    "lgbm_clf = LGBMClassifier(\n",
    "    num_leaves=20,\n",
    "    n_estimators=500,\n",
    "    learning_rate=0.1,\n",
    "    reg_alpha=0.1,\n",
    "    min_child_samples=3,\n",
    "    colsample_bytree=0.5,\n",
    "    random_state=42,\n",
    "    n_jobs=-1\n",
    ")\n",
    "\n",
    "lgbm_clf.fit(X_train, y_train_cls)\n",
    "y_pred_lgb_clf = lgbm_clf.predict(X_test)\n",
    "\n",
    "\n",
    "print(\"Accuracy:\", accuracy_score(y_test_cls, y_pred_lgb_clf))\n",
    "print(classification_report(y_test_cls, y_pred_lgb_clf, target_names=emotion_cols))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a87645c5",
   "metadata": {
    "papermill": {
     "duration": 0.007444,
     "end_time": "2025-06-02T10:10:08.658536",
     "exception": false,
     "start_time": "2025-06-02T10:10:08.651092",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Support Vector Machines (SVM)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "9aa8eb19",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-02T10:10:08.675247Z",
     "iopub.status.busy": "2025-06-02T10:10:08.674951Z",
     "iopub.status.idle": "2025-06-02T10:11:11.566316Z",
     "shell.execute_reply": "2025-06-02T10:11:11.564735Z"
    },
    "papermill": {
     "duration": 62.902388,
     "end_time": "2025-06-02T10:11:11.568746",
     "exception": false,
     "start_time": "2025-06-02T10:10:08.666358",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.11/dist-packages/sklearn/svm/_base.py:299: ConvergenceWarning: Solver terminated early (max_iter=10000).  Consider pre-processing your data with StandardScaler or MinMaxScaler.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.8083859332732192\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         joy       0.84      0.85      0.85       615\n",
      "     sadness       0.78      0.78      0.78       475\n",
      "       anger       0.85      0.77      0.81       526\n",
      "        fear       0.77      0.82      0.79       602\n",
      "\n",
      "    accuracy                           0.81      2218\n",
      "   macro avg       0.81      0.81      0.81      2218\n",
      "weighted avg       0.81      0.81      0.81      2218\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.svm import SVC\n",
    "from sklearn.multioutput import MultiOutputClassifier\n",
    "\n",
    "svm_model = SVC(kernel='linear', C=1.0, probability=True, max_iter=10000, random_state=42)\n",
    "svm_model.fit(X_train, y_train_cls)\n",
    "y_pred_svm = svm_model.predict(X_test)\n",
    "\n",
    "print(\"Accuracy:\", accuracy_score(y_test_cls, y_pred_svm))\n",
    "print(classification_report(y_test_cls, y_pred_svm, target_names=emotion_cols))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "abff64ce",
   "metadata": {
    "papermill": {
     "duration": 0.007681,
     "end_time": "2025-06-02T10:11:11.584502",
     "exception": false,
     "start_time": "2025-06-02T10:11:11.576821",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Ensemble Logistic Regression & LightGBM (BEST)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "024e07cc",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-02T10:11:11.603895Z",
     "iopub.status.busy": "2025-06-02T10:11:11.603580Z",
     "iopub.status.idle": "2025-06-02T10:11:39.376664Z",
     "shell.execute_reply": "2025-06-02T10:11:39.375632Z"
    },
    "papermill": {
     "duration": 27.78487,
     "end_time": "2025-06-02T10:11:39.378137",
     "exception": false,
     "start_time": "2025-06-02T10:11:11.593267",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.237482 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 28074\n",
      "[LightGBM] [Info] Number of data points in the train set: 8872, number of used features: 4885\n",
      "[LightGBM] [Info] Start training from score -1.326784\n",
      "[LightGBM] [Info] Start training from score -1.548442\n",
      "[LightGBM] [Info] Start training from score -1.439535\n",
      "[LightGBM] [Info] Start training from score -1.255076\n",
      "Accuracy: 0.8237150586113616\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         joy       0.87      0.87      0.87       615\n",
      "     sadness       0.82      0.79      0.80       475\n",
      "       anger       0.85      0.79      0.82       526\n",
      "        fear       0.77      0.82      0.79       602\n",
      "\n",
      "    accuracy                           0.82      2218\n",
      "   macro avg       0.83      0.82      0.82      2218\n",
      "weighted avg       0.82      0.82      0.82      2218\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.11/dist-packages/lightgbm/basic.py:1218: UserWarning: Converting data to scipy sparse matrix.\n",
      "  _log_warning(\"Converting data to scipy sparse matrix.\")\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import VotingClassifier\n",
    "\n",
    "ensemble_clf = VotingClassifier(\n",
    "    estimators=[\n",
    "        ('lr', logR),\n",
    "        ('lgbm', lgbm_clf),\n",
    "    ],\n",
    "    voting='soft'\n",
    ")\n",
    "ensemble_clf.fit(X_train, y_train_cls)\n",
    "y_pred_ensemble = ensemble_clf.predict(X_test)\n",
    "\n",
    "print(\"Accuracy:\", accuracy_score(y_test_cls, y_pred_ensemble))\n",
    "print(classification_report(y_test_cls, y_pred_ensemble, target_names=emotion_cols))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "affa3b04",
   "metadata": {
    "papermill": {
     "duration": 0.007871,
     "end_time": "2025-06-02T10:11:39.394951",
     "exception": false,
     "start_time": "2025-06-02T10:11:39.387080",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Final Model Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "71498690",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-02T10:11:39.414121Z",
     "iopub.status.busy": "2025-06-02T10:11:39.413774Z",
     "iopub.status.idle": "2025-06-02T10:11:39.424009Z",
     "shell.execute_reply": "2025-06-02T10:11:39.422772Z"
    },
    "papermill": {
     "duration": 0.023025,
     "end_time": "2025-06-02T10:11:39.426072",
     "exception": false,
     "start_time": "2025-06-02T10:11:39.403047",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def EmoIntPipeline(texts):\n",
    "    if isinstance(texts, str):\n",
    "        texts = [texts]\n",
    "\n",
    "    # Clean text and extract tfidf\n",
    "    texts = [clean_text(t) for t in texts]\n",
    "    tfidf_feat = tfidf.transform(texts)\n",
    "\n",
    "    # Extract lexicons\n",
    "    lex_feat = np.array([extract_lex(t, nrc_lexicon) for t in texts])\n",
    "    vad_feat = np.array([extract_vad(t, nrc_vad_lexicon) for t in texts])\n",
    "    hash_feat = np.array([extract_hash_emo(t, hash_emo_lex) for t in texts])\n",
    "\n",
    "    # Scale lexicons\n",
    "    lex_feat = scaler_lex.transform(lex_feat)\n",
    "    vad_feat = scaler_vad.transform(vad_feat)\n",
    "    hash_feat = scaler_hash.transform(hash_feat)\n",
    "\n",
    "    # Combine lexicons\n",
    "    combined_lex = np.concatenate([vad_feat, lex_feat, hash_feat], axis=1)\n",
    "    combined_feat = hstack([tfidf_feat, combined_lex])\n",
    "\n",
    "    # Classification\n",
    "    probs = ensemble_clf.predict_proba(combined_feat)\n",
    "    confidence_scores = np.max(probs, axis=1)\n",
    "    predicted_classes = np.argmax(probs, axis=1)\n",
    "    pred_emotion = [emotion_cols[i] for i in predicted_classes]\n",
    "\n",
    "    # Regression\n",
    "    pred_reg_all = ensemble_reg.predict(combined_feat)\n",
    "    pred_intensity = [pred[i] for pred, i in zip(pred_reg_all, predicted_classes)]\n",
    "\n",
    "    rets = []\n",
    "    for text, em, score, conf in zip(texts, pred_emotion, pred_intensity, confidence_scores):\n",
    "        if conf > 0.4 and score < 0.4:\n",
    "            boost = 0.3 * conf\n",
    "            score = min(score + boost, 1.0)\n",
    "        rets.append((text, em, score, conf))\n",
    "\n",
    "    return rets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "d028a788",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-02T10:11:39.446030Z",
     "iopub.status.busy": "2025-06-02T10:11:39.445687Z",
     "iopub.status.idle": "2025-06-02T10:11:39.472507Z",
     "shell.execute_reply": "2025-06-02T10:11:39.469190Z"
    },
    "papermill": {
     "duration": 0.039056,
     "end_time": "2025-06-02T10:11:39.474346",
     "exception": false,
     "start_time": "2025-06-02T10:11:39.435290",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Text: i am so happy and grateful today!\n",
      "Emotion: joy\n",
      "Intensity: 0.884\n",
      "----------------------------------------\n",
      "Text: i am kind of happy today, but at the same time life is so bland\n",
      "Emotion: joy\n",
      "Intensity: 0.646\n",
      "----------------------------------------\n",
      "Text: i might be happy today, but it is just a normal day.\n",
      "Emotion: joy\n",
      "Intensity: 0.570\n",
      "----------------------------------------\n",
      "Text: this makes me so angry, i cannot believe they did that.\n",
      "Emotion: anger\n",
      "Intensity: 0.657\n",
      "----------------------------------------\n",
      "Text: i am angry, but i think i can tolerate their behavior.\n",
      "Emotion: anger\n",
      "Intensity: 0.460\n",
      "----------------------------------------\n",
      "Text: i am extremely furious, he never get things right.\n",
      "Emotion: anger\n",
      "Intensity: 0.526\n",
      "----------------------------------------\n",
      "Text: i am feeling a bit down today, things are not going as planned.\n",
      "Emotion: sadness\n",
      "Intensity: 0.365\n",
      "----------------------------------------\n",
      "Text: my girlfriend just dumped me, i do not know what to do with my life anymore.\n",
      "Emotion: sadness\n",
      "Intensity: 0.411\n",
      "----------------------------------------\n",
      "Text: i am crying my eyes out now, a family member of mine just passed away.\n",
      "Emotion: sadness\n",
      "Intensity: 0.423\n",
      "----------------------------------------\n",
      "Text: that movie was terrifying, i could not sleep all night.\n",
      "Emotion: fear\n",
      "Intensity: 0.572\n",
      "----------------------------------------\n",
      "Text: the haunted house was scary, but we had so much fun\n",
      "Emotion: fear\n",
      "Intensity: 0.437\n",
      "----------------------------------------\n",
      "Text: that scared me so much, i almost had a heart attack.\n",
      "Emotion: fear\n",
      "Intensity: 0.573\n",
      "----------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.11/dist-packages/lightgbm/basic.py:1218: UserWarning: Converting data to scipy sparse matrix.\n",
      "  _log_warning(\"Converting data to scipy sparse matrix.\")\n"
     ]
    }
   ],
   "source": [
    "sample_text = [\n",
    "    \"I'm so happy and grateful today!\",\n",
    "    \"I'm kinda happy today, but at the same time life is so bland\",\n",
    "    \"I might be happy today, but it's just a normal day.\",\n",
    "    \"This makes me so angry, I can't believe they did that.\",\n",
    "    \"I'm angry, but I think I can tolerate their behavior.\",\n",
    "    \"I'm extremely furious, he never get things right.\",\n",
    "    \"I'm feeling a bit down today, things aren't going as planned.\",\n",
    "    \"My girlfriend just dumped me, I don't know what to do with my life anymore.\",\n",
    "    \"I'm crying my eyes out now, a family member of mine just passed away.\",\n",
    "    \"That movie was terrifying, I couldn't sleep all night.\",\n",
    "    \"The haunted house was scary, but we had so much fun\",\n",
    "    \"That scared me so much, I almost had a heart attack.\"\n",
    "]\n",
    "rets = EmoIntPipeline(sample_text)\n",
    "for text, emotion, intensity, confidence in rets:\n",
    "    print(f\"Text: {text}\")\n",
    "    print(f\"Emotion: {emotion}\")\n",
    "    print(f\"Intensity: {intensity:.3f}\")\n",
    "    # print(f\"Probability: {confidence:.3f}\")\n",
    "    print(\"-\" * 40)"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "none",
   "dataSources": [
    {
     "datasetId": 7434328,
     "sourceId": 11833570,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 7434335,
     "sourceId": 11833579,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 31040,
   "isGpuEnabled": false,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 190.976005,
   "end_time": "2025-06-02T10:11:40.606468",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2025-06-02T10:08:29.630463",
   "version": "2.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
